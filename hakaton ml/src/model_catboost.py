"""
CatBoost –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
"""
import pandas as pd
import numpy as np
from catboost import CatBoostClassifier
import joblib
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).parent.parent))
from src.utils import log_info, load_dataframe, MODELS_DIR
from src.validation import evaluate_model, train_test_split_stratified

def prepare_features_catboost(features_df, target_dict=None):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è CatBoost
    
    Args:
        features_df: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        target_dict: —Å–ª–æ–≤–∞—Ä—å {student_id: target} –¥–ª—è train
    
    Returns:
        X: –ø—Ä–∏–∑–Ω–∞–∫–∏
        y: —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (–µ—Å–ª–∏ target_dict –ø–µ—Ä–µ–¥–∞–Ω)
        feature_names: –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        cat_features: –∏–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """
    # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ-–ø—Ä–∏–∑–Ω–∞–∫–∏
    exclude_cols = ['student_id', '–§–∞–∫—É–ª—å—Ç–µ—Ç', '–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '–≥–æ–¥ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è']
    feature_cols = [col for col in features_df.columns if col not in exclude_cols]
    
    X = features_df[feature_cols].fillna(0)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    cat_features = []
    for i, col in enumerate(feature_cols):
        if 'encoded' in col.lower() or col in ['faculty_encoded', 'direction_encoded']:
            cat_features.append(i)
    
    if target_dict is not None:
        y = features_df['student_id'].map(target_dict).fillna(0).astype(int)
        return X, y, feature_cols, cat_features
    else:
        return X, None, feature_cols, cat_features

def train_catboost(X_train, y_train, X_val=None, y_val=None, cat_features=None):
    """
    –û–±—É—á–µ–Ω–∏–µ CatBoost –º–æ–¥–µ–ª–∏
    
    Args:
        X_train: –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        y_train: —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        X_val: –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        y_val: —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        cat_features: –∏–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    
    Returns:
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        metrics: –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏
    """
    log_info("\n" + "=" * 60)
    log_info("–û–ë–£–ß–ï–ù–ò–ï CATBOOST –ú–û–î–ï–õ–ò")
    log_info("=" * 60)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    model = CatBoostClassifier(
        iterations=1000,
        learning_rate=0.1,
        depth=6,
        l2_leaf_reg=3,
        loss_function='Logloss',
        eval_metric='AUC',
        random_seed=42,
        verbose=100,
        early_stopping_rounds=100,
        class_weights=[1, 1]  # –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
    )
    
    log_info("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    if X_val is not None and y_val is not None:
        model.fit(
            X_train, y_train,
            eval_set=(X_val, y_val),
            cat_features=cat_features if cat_features else None,
            use_best_model=True,
            verbose=100
        )
    else:
        model.fit(
            X_train, y_train,
            cat_features=cat_features if cat_features else None,
            verbose=100
        )
    
    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    metrics = evaluate_model(model, X_train, y_train, X_val, y_val, cv=5)
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    log_info("\nüìä –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    for i, row in feature_importance.head(10).iterrows():
        log_info(f"  {row['feature']}: {row['importance']:.4f}")
    
    metrics['feature_importance'] = feature_importance.to_dict('records')
    
    return model, metrics

if __name__ == "__main__":
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    log_info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç (–µ—Å–ª–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –ù–∏–∫–∏—Ç–æ–π)
    try:
        train_final = load_dataframe("train_final.parquet")
        log_info("  –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π train –¥–∞—Ç–∞—Å–µ—Ç (–ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –ù–∏–∫–∏—Ç–æ–π)")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        X_train = train_final.drop(['student_id', 'target'], axis=1)
        y_train = train_final['target']
        feature_cols = list(X_train.columns)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        cat_features = []
        for i, col in enumerate(feature_cols):
            if 'encoded' in col.lower() or col in ['faculty_encoded', 'direction_encoded']:
                cat_features.append(i)
        
    except FileNotFoundError:
        # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–±
        log_info("  –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–±...")
        features_df = load_dataframe("features.parquet")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ target –¥–ª—è train
        train_target = load_dataframe("train_target.parquet")
        target_dict = dict(zip(train_target['–ò–î'], train_target['target']))
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train –∏ test —Å—Ç—É–¥–µ–Ω—Ç–æ–≤
        train_students = set(train_target['–ò–î'].unique())
        train_features = features_df[features_df['student_id'].isin(train_students)].copy()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X_train, y_train, feature_cols, cat_features = prepare_features_catboost(train_features, target_dict)
    
    log_info(f"  Train —Ä–∞–∑–º–µ—Ä: {len(X_train):,} —Å—Ç—É–¥–µ–Ω—Ç–æ–≤")
    log_info(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")
    log_info(f"  –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(cat_features)}")
    log_info(f"  –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {y_train.value_counts().to_dict()}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val
    X_train_split, X_val, y_train_split, y_val = train_test_split_stratified(
        X_train, y_train, test_size=0.2, random_state=42
    )
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model, metrics = train_catboost(X_train_split, y_train_split, X_val, y_val, cat_features)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model_path = MODELS_DIR / "catboost_model.pkl"
    joblib.dump(model, model_path)
    log_info(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    import json
    model_info = {
        'feature_names': feature_cols,
        'cat_features': cat_features,
        'metrics': metrics
    }
    with open(Path(__file__).parent.parent / "output" / "catboost_info.json", 'w') as f:
        json.dump(model_info, f, indent=2, default=str)
    
    log_info("\n‚úÖ CatBoost –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

