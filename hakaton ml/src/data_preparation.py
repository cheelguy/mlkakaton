"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∑–∞–¥–∞—á–∏ –ù–∏–∫–∏—Ç—ã):
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å data.csv
2. –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–∏—Ç—å marking.csv
3. –£–¥–∞–ª–∏—Ç—å —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ —Ç–µ—Å—Ç–∞ (–≤—ã–ø—É—Å–∫=None) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
5. –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–ø–∏—Å–æ–∫ –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö —Ñ–∏—á
6. –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–∞–º–∏ —Ñ–∏—á–∏
7. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ —É—Ç–µ—á–∫–∏
8. –ü–µ—Ä–µ–¥–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –î–∏–º–µ
"""
import pandas as pd
import numpy as np
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).parent.parent))
from src.utils import log_info, save_dataframe, load_dataframe, check_data_leakage
from src.load_data import load_raw_data, analyze_data
from src.preprocess import preprocess_data
from src.feature_engineering import create_student_features, merge_with_marking

def step1_analyze_data_csv(data_df):
    """
    –®–∞–≥ 1: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å data.csv
    """
    log_info("\n" + "=" * 60)
    log_info("–®–ê–ì 1: –ê–ù–ê–õ–ò–ó data.csv")
    log_info("=" * 60)
    
    log_info(f"\nüìä –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    log_info(f"  –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(data_df):,}")
    log_info(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—É–¥–µ–Ω—Ç–æ–≤: {data_df['PK'].nunique():,}")
    log_info(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–µ–º–µ—Å—Ç—Ä–æ–≤: {sorted(data_df['SEMESTER'].unique())}")
    log_info(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∏—Å—Ü–∏–ø–ª–∏–Ω: {data_df['DNAME'].nunique():,}")
    
    log_info(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–µ–º–µ—Å—Ç—Ä–∞–º:")
    semester_dist = data_df['SEMESTER'].value_counts().sort_index()
    for sem, count in semester_dist.items():
        pct = count / len(data_df) * 100
        log_info(f"  –°–µ–º–µ—Å—Ç—Ä {sem}: {count:,} ({pct:.1f}%)")
    
    log_info(f"\nüìä –¢–∏–ø—ã –æ—Ü–µ–Ω–æ–∫:")
    type_dist = data_df['TYPE'].value_counts()
    for typ, count in type_dist.items():
        pct = count / len(data_df) * 100
        log_info(f"  {typ}: {count:,} ({pct:.1f}%)")
    
    return data_df

def step2_join_marking(data_df, marking_df):
    """
    –®–∞–≥ 2: –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–∏—Ç—å marking.csv
    """
    log_info("\n" + "=" * 60)
    log_info("–®–ê–ì 2: –ü–†–ò–°–û–ï–î–ò–ù–ï–ù–ò–ï marking.csv")
    log_info("=" * 60)
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    marking_renamed = marking_df.rename(columns={'–ò–î': 'PK'})
    
    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—Ç—É–¥–µ–Ω—Ç–∞ (—Å–∞–º—É—é –∞–∫—Ç—É–∞–ª—å–Ω—É—é)
    if '–¥–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è' in marking_renamed.columns:
        marking_latest = marking_renamed.sort_values('–¥–∞—Ç–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è', ascending=False).drop_duplicates(subset=['PK'], keep='first')
    else:
        marking_latest = marking_renamed.drop_duplicates(subset=['PK'], keep='first')
    
    log_info(f"  –ó–∞–ø–∏—Å–µ–π –≤ marking: {len(marking_df):,}")
    log_info(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –≤ marking: {marking_latest['PK'].nunique():,}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
    students_in_data = set(data_df['PK'].unique())
    students_in_marking = set(marking_latest['PK'].unique())
    common = students_in_data & students_in_marking
    
    log_info(f"\n  –°—Ç—É–¥–µ–Ω—Ç–æ–≤ –≤ data.csv: {len(students_in_data):,}")
    log_info(f"  –°—Ç—É–¥–µ–Ω—Ç–æ–≤ –≤ marking.csv: {len(students_in_marking):,}")
    log_info(f"  –û–±—â–∏—Ö —Å—Ç—É–¥–µ–Ω—Ç–æ–≤: {len(common):,}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º marking –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    save_dataframe(marking_latest, "marking_processed.parquet")
    
    return marking_latest

def step3_remove_test_students(data_df, marking_df, sample_submission_df):
    """
    –®–∞–≥ 3: –£–¥–∞–ª–∏—Ç—å —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ —Ç–µ—Å—Ç–∞ (–≤—ã–ø—É—Å–∫=None) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    """
    log_info("\n" + "=" * 60)
    log_info("–®–ê–ì 3: –£–î–ê–õ–ï–ù–ò–ï –°–¢–£–î–ï–ù–¢–û–í –¢–ï–°–¢–ê")
    log_info("=" * 60)
    
    # –ü–æ–ª—É—á–∞–µ–º ID —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞
    test_student_ids = set(sample_submission_df['ID'].unique())
    log_info(f"  –°—Ç—É–¥–µ–Ω—Ç–æ–≤ –≤ —Ç–µ—Å—Ç–µ: {len(test_student_ids):,}")
    
    # –°—Ç—É–¥–µ–Ω—Ç—ã —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–¥–ª—è train)
    marking_with_target = marking_df[marking_df['–≤—ã–ø—É—Å–∫'].notna()].copy()
    train_student_ids = set(marking_with_target['PK'].unique())
    
    # –ò—Å–∫–ª—é—á–∞–µ–º —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–µ—Å—Ç–∞
    train_student_ids = train_student_ids - test_student_ids
    log_info(f"  –°—Ç—É–¥–µ–Ω—Ç–æ–≤ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {len(marking_with_target['PK'].unique()):,}")
    log_info(f"  –°—Ç—É–¥–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è —Ç–µ—Å—Ç–∞): {len(train_student_ids):,}")
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ: —Ç–æ–ª—å–∫–æ —Å–µ–º–µ—Å—Ç—Ä—ã 1-4 –¥–ª—è train —Å—Ç—É–¥–µ–Ω—Ç–æ–≤
    train_data = data_df[
        (data_df['PK'].isin(train_student_ids)) & 
        (data_df['SEMESTER'] <= 4)
    ].copy()
    
    log_info(f"  –ó–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(train_data):,}")
    log_info(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –≤ train –¥–∞–Ω–Ω—ã—Ö: {train_data['PK'].nunique():,}")
    
    return train_data, train_student_ids, test_student_ids

def step4_check_missing_data(data_df):
    """
    –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    """
    log_info("\n" + "=" * 60)
    log_info("–®–ê–ì 4: –ü–†–û–í–ï–†–ö–ê –û–¢–°–£–¢–°–¢–í–£–Æ–©–ò–• –î–ê–ù–ù–´–•")
    log_info("=" * 60)
    
    missing_summary = {}
    
    for col in data_df.columns:
        missing_count = data_df[col].isnull().sum()
        if missing_count > 0:
            missing_pct = missing_count / len(data_df) * 100
            missing_summary[col] = {
                'count': missing_count,
                'percentage': missing_pct
            }
            log_info(f"  {col}: {missing_count:,} ({missing_pct:.2f}%)")
    
    if not missing_summary:
        log_info("  ‚úÖ –ü—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ!")
    else:
        log_info(f"\n  –í—Å–µ–≥–æ –∫–æ–ª–æ–Ω–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏: {len(missing_summary)}")
    
    return missing_summary

def step5_build_safe_features_list(data_df, marking_df):
    """
    –®–∞–≥ 5: –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–ø–∏—Å–æ–∫ –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö —Ñ–∏—á
    
    –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ —Ñ–∏—á–∏ = —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–µ—Ä–≤—ã–µ 2 –∫—É—Ä—Å–∞ (—Å–µ–º–µ—Å—Ç—Ä—ã 1-4)
    """
    log_info("\n" + "=" * 60)
    log_info("–®–ê–ì 5: –ü–û–°–¢–†–û–ï–ù–ò–ï –°–ü–ò–°–ö–ê –ë–ï–ó–û–ü–ê–°–ù–´–• –§–ò–ß")
    log_info("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Å–µ–º–µ—Å—Ç—Ä—ã 1-4
    max_semester = data_df['SEMESTER'].max()
    min_semester = data_df['SEMESTER'].min()
    
    log_info(f"  –î–∏–∞–ø–∞–∑–æ–Ω —Å–µ–º–µ—Å—Ç—Ä–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö: {min_semester} - {max_semester}")
    
    if max_semester > 4:
        log_info(f"  ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ï—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –∑–∞ —Å–µ–º–µ—Å—Ç—Ä—ã > 4, –æ–Ω–∏ –±—É–¥—É—Ç –∏—Å–∫–ª—é—á–µ–Ω—ã!")
        data_safe = data_df[data_df['SEMESTER'] <= 4].copy()
    else:
        data_safe = data_df.copy()
    
    log_info(f"  –ë–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (—Å–µ–º–µ—Å—Ç—Ä—ã 1-4): {len(data_safe):,}")
    
    # –°–ø–∏—Å–æ–∫ –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–≤—Å–µ, —á—Ç–æ –º–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å –∏–∑ —Å–µ–º–µ—Å—Ç—Ä–æ–≤ 1-4)
    safe_features_categories = [
        "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –æ—Ü–µ–Ω–∫–∞–º (—Å—Ä–µ–¥–Ω–µ–µ, –º–µ–¥–∏–∞–Ω–∞, –º–∏–Ω, –º–∞–∫—Å, std)",
        "–î–∏–Ω–∞–º–∏–∫–∞ –æ—Ü–µ–Ω–æ–∫ –ø–æ —Å–µ–º–µ—Å—Ç—Ä–∞–º 1-4",
        "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º –æ—Ü–µ–Ω–æ–∫ (—ç–∫–∑–∞–º–µ–Ω—ã, –∑–∞—á–µ—Ç—ã, –ø—Ä–∞–∫—Ç–∏–∫–∏)",
        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω",
        "–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫",
        "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—É–¥–µ–Ω—Ç–µ (—Ñ–∞–∫—É–ª—å—Ç–µ—Ç, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, –≥–æ–¥ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è)"
    ]
    
    log_info(f"\n  –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    for i, cat in enumerate(safe_features_categories, 1):
        log_info(f"    {i}. {cat}")
    
    return data_safe

def step6_build_features(data_df, marking_df, student_ids=None):
    """
    –®–∞–≥ 6: –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–∞–º–∏ —Ñ–∏—á–∏
    """
    log_info("\n" + "=" * 60)
    log_info("–®–ê–ì 6: –ü–û–°–¢–†–û–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í")
    log_info("=" * 60)
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    log_info("\n  –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    processed_df = preprocess_data(data_df)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    log_info("\n  –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    features_df = create_student_features(processed_df, student_ids)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å marking
    log_info("\n  –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å marking...")
    features_df = merge_with_marking(features_df, marking_df)
    
    log_info(f"\n  ‚úÖ –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features_df.columns) - 1}")
    log_info(f"  ‚úÖ –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤: {len(features_df):,}")
    
    return features_df, processed_df

def step7_check_leakage(train_features, test_features):
    """
    –®–∞–≥ 7: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ —É—Ç–µ—á–∫–∏
    """
    log_info("\n" + "=" * 60)
    log_info("–®–ê–ì 7: –ü–†–û–í–ï–†–ö–ê –ù–ê –£–¢–ï–ß–ö–ò")
    log_info("=" * 60)
    
    # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ-–ø—Ä–∏–∑–Ω–∞–∫–∏
    exclude_cols = ['student_id', '–§–∞–∫—É–ª—å—Ç–µ—Ç', '–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '–≥–æ–¥ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è', '–≤—ã–ø—É—Å–∫', 'target']
    feature_cols = [col for col in train_features.columns if col not in exclude_cols]
    
    log_info(f"  –ü—Ä–æ–≤–µ—Ä–∫–∞ {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    
    warnings = check_data_leakage(train_features, test_features, feature_cols)
    
    if warnings:
        log_info(f"\n  ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(warnings)} –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —É—Ç–µ—á–∫–∞—Ö!")
        log_info("  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —ç—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤—Ä—É—á–Ω—É—é.")
    else:
        log_info(f"\n  ‚úÖ –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø—Ä–æ—à–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ —É—Ç–µ—á–∫–∏!")
    
    return warnings, feature_cols

def step8_prepare_final_dataset(train_features, test_features, train_target, feature_cols):
    """
    –®–∞–≥ 8: –ü–µ—Ä–µ–¥–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –î–∏–º–µ
    """
    log_info("\n" + "=" * 60)
    log_info("–®–ê–ì 8: –ü–û–î–ì–û–¢–û–í–ö–ê –§–ò–ù–ê–õ–¨–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê")
    log_info("=" * 60)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ train –¥–∞—Ç–∞—Å–µ—Ç–∞
    train_final = train_features[['student_id'] + feature_cols].copy()
    train_final = train_final.merge(
        train_target[['–ò–î', 'target']].rename(columns={'–ò–î': 'student_id'}),
        on='student_id',
        how='inner'
    )
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ test –¥–∞—Ç–∞—Å–µ—Ç–∞
    test_final = test_features[['student_id'] + feature_cols].copy()
    
    log_info(f"\n  Train –¥–∞—Ç–∞—Å–µ—Ç:")
    log_info(f"    –°—Ç—É–¥–µ–Ω—Ç–æ–≤: {len(train_final):,}")
    log_info(f"    –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")
    log_info(f"    –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {train_final['target'].value_counts().to_dict()}")
    
    log_info(f"\n  Test –¥–∞—Ç–∞—Å–µ—Ç:")
    log_info(f"    –°—Ç—É–¥–µ–Ω—Ç–æ–≤: {len(test_final):,}")
    log_info(f"    –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    save_dataframe(train_final, "train_final.parquet")
    save_dataframe(test_final, "test_final.parquet")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    import json
    features_info = {
        'feature_names': feature_cols,
        'n_features': len(feature_cols),
        'n_train': len(train_final),
        'n_test': len(test_final)
    }
    with open(Path(__file__).parent.parent / "output" / "features_info.json", 'w') as f:
        json.dump(features_info, f, indent=2)
    
    log_info(f"\n  ‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    log_info(f"    - data/processed/train_final.parquet")
    log_info(f"    - data/processed/test_final.parquet")
    log_info(f"    - output/features_info.json")
    
    log_info("\n" + "=" * 60)
    log_info("‚úÖ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù–ê!")
    log_info("   –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –î–∏–º–µ (Models + Validation)")
    log_info("=" * 60)
    
    return train_final, test_final

def run_full_data_preparation():
    """
    –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö (–≤—Å–µ —à–∞–≥–∏ –ù–∏–∫–∏—Ç—ã)
    """
    log_info("\n" + "=" * 80)
    log_info("–ü–û–õ–ù–ê–Ø –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• (–ó–ê–î–ê–ß–ò –ù–ò–ö–ò–¢–´)")
    log_info("=" * 80)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    data_df, marking_df, sample_submission_df = load_raw_data()
    
    # –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ data.csv
    data_df = step1_analyze_data_csv(data_df)
    
    # –®–∞–≥ 2: –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ marking.csv
    marking_processed = step2_join_marking(data_df, marking_df)
    
    # –®–∞–≥ 3: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ —Ç–µ—Å—Ç–∞
    train_data, train_student_ids, test_student_ids = step3_remove_test_students(
        data_df, marking_processed, sample_submission_df
    )
    
    # –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    missing_summary = step4_check_missing_data(train_data)
    
    # –®–∞–≥ 5: –°–ø–∏—Å–æ–∫ –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö —Ñ–∏—á
    data_safe = step5_build_safe_features_list(train_data, marking_processed)
    
    # –®–∞–≥ 6: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    train_features, processed_df = step6_build_features(
        data_safe, marking_processed, train_student_ids
    )
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–∞ —Ç–æ–∂–µ
    test_data = data_df[
        (data_df['PK'].isin(test_student_ids)) & 
        (data_df['SEMESTER'] <= 4)
    ].copy()
    test_processed = preprocess_data(test_data)
    test_features, _ = step6_build_features(
        test_processed, marking_processed, test_student_ids
    )
    
    # –®–∞–≥ 7: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ç–µ—á–∫–∏
    warnings, feature_cols = step7_check_leakage(train_features, test_features)
    
    # –®–∞–≥ 8: –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    train_target = marking_processed[
        marking_processed['PK'].isin(train_student_ids)
    ][['PK', '–≤—ã–ø—É—Å–∫']].copy()
    train_target['target'] = (train_target['–≤—ã–ø—É—Å–∫'] == '–≤—ã–ø—É—Å—Ç–∏–ª—Å—è').astype(int)
    train_target = train_target.rename(columns={'PK': 'student_id'})
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ñ—É–Ω–∫—Ü–∏–µ–π
    train_target_for_func = train_target.rename(columns={'student_id': '–ò–î'})
    
    train_final, test_final = step8_prepare_final_dataset(
        train_features, test_features, train_target_for_func, feature_cols
    )
    
    return train_final, test_final, feature_cols

if __name__ == "__main__":
    train_final, test_final, feature_cols = run_full_data_preparation()
    
    log_info("\n‚úÖ –í—Å–µ —à–∞–≥–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!")
    log_info("   –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π (–∑–∞–¥–∞—á–∏ –î–∏–º—ã)")

